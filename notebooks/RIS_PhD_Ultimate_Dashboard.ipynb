{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì RIS PhD Ultimate Research Dashboard\n",
    "\n",
    "## Comprehensive Research Platform for RIS Probe-Based ML\n",
    "\n",
    "**Version:** 2.0.0 (Phase 2 - MATLAB Integration Active)\n",
    "**Author:** Your Name\n",
    "**Last Updated:** January 2025\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Features\n",
    "\n",
    "#### Phase 1 (Core)\n",
    "- ‚úÖ 5 Configuration Tabs (System, Physics, Model, Training, Evaluation, Visualization)\n",
    "- ‚úÖ 19+ Pre-defined Model Architectures + Custom\n",
    "- ‚úÖ 6 Probe Types (continuous, binary, 2bit, hadamard, sobol, halton)\n",
    "- ‚úÖ Experiment Stacking & Batch Execution\n",
    "- ‚úÖ Transfer Learning Support\n",
    "- ‚úÖ 25+ Plot Types with Interactive Visualization\n",
    "- ‚úÖ Multi-Model & Multi-Seed Comparison\n",
    "- ‚úÖ Config Save/Load (JSON)\n",
    "\n",
    "#### Phase 2 (MATLAB Integration)\n",
    "- ‚úÖ Dual Backend System (Python / MATLAB)\n",
    "- ‚úÖ MATLAB Engine Integration\n",
    "- ‚úÖ MathWorks Verified Toolboxes (Communications, 5G)\n",
    "- ‚úÖ Multiple Channel Scenarios (Rayleigh, CDL-RIS, TDL, Rician)\n",
    "- ‚úÖ Automatic Fallback to Python\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Quick Start Guide\n",
    "\n",
    "1. **Run Cell 2** - Setup and verify environment\n",
    "2. **Run Cell 3** - Initialize dashboard\n",
    "3. **Configure** - Use tabs to set parameters\n",
    "4. **Add to Stack** - Build experiment queue\n",
    "5. **Run Stack** - Execute all experiments\n",
    "6. **Run Cell 4** - View results and analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup & Installation Check\n",
    "\n",
    "Verify environment setup and dependencies."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from networkx.algorithms.tournament import score_sequence\n",
    "from scipy.cluster.hierarchy import weighted\n",
    "from torch.fx.experimental.symbolic_shapes import lru_cache\n",
    "\n",
    "from tests.diagnose_runner_integration import criterion\n",
    "# ============================================================================\n",
    "# CELL 2: ENVIRONMENT SETUP & VERIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure project root is in path\n",
    "project_root = Path(os.getcwd()).parent if 'notebooks' in os.getcwd() else Path(os.getcwd())\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîß RIS RESEARCH PLATFORM - ENVIRONMENT CHECK\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Check Python version\n",
    "print(\"[1/6] Python Version:\")\n",
    "print(f\"   ‚úì Python {sys.version.split()[0]}\")\n",
    "print()\n",
    "\n",
    "# Check core dependencies\n",
    "print(\"[2/6] Core Dependencies:\")\n",
    "required_packages = [\n",
    "    'numpy', 'torch', 'matplotlib', 'seaborn',\n",
    "    'scipy', 'pandas', 'ipywidgets'\n",
    "]\n",
    "for pkg in required_packages:\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "        print(f\"   ‚úì {pkg}\")\n",
    "    except ImportError:\n",
    "        print(f\"   ‚úó {pkg} - MISSING!\")\n",
    "print()\n",
    "\n",
    "# Check project modules\n",
    "print(\"[3/6] Project Modules:\")\n",
    "project_modules = ['config', 'data', 'models', 'training', 'evaluation', 'dashboard', 'physics']\n",
    "for mod in project_modules:\n",
    "    try:\n",
    "        __import__(mod)\n",
    "        print(f\"   ‚úì {mod}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"   ‚úó {mod} - ERROR: {e}\")\n",
    "print()\n",
    "\n",
    "# Check MATLAB availability (Phase 2)\n",
    "print(\"[4/6] MATLAB Backend (Phase 2):\")\n",
    "try:\n",
    "    import matlab.engine\n",
    "    print(\"   ‚úì MATLAB Engine for Python - AVAILABLE\")\n",
    "    print(\"   ‚úì Phase 2 features: ENABLED\")\n",
    "except ImportError:\n",
    "    print(\"   ‚ö† MATLAB Engine - NOT AVAILABLE\")\n",
    "    print(\"   ‚Ñπ Phase 2 will use Python fallback\")\n",
    "print()\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"[5/6] GPU Acceleration:\")\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   ‚úì CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"   ‚Ñπ CPU only (no GPU detected)\")\n",
    "except:\n",
    "    print(\"   ‚Ñπ CPU only\")\n",
    "print()\n",
    "\n",
    "# Verify dashboard components\n",
    "print(\"[6/6] Dashboard Components:\")\n",
    "try:\n",
    "    from dashboard import create_complete_interface\n",
    "    from dashboard.callbacks import setup_all_callbacks, setup_experiment_handlers\n",
    "    print(\"   ‚úì Dashboard interface\")\n",
    "    print(\"   ‚úì Callback system\")\n",
    "    print(\"   ‚úì Experiment runner\")\n",
    "except ImportError as e:\n",
    "    print(f\"   ‚úó Dashboard ERROR: {e}\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ ENVIRONMENT CHECK COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"Ready to proceed! Run Cell 3 to launch dashboard.\")\n",
    "print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# CELL 3: DASHBOARD INITIALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from dashboard import create_complete_interface\n",
    "from dashboard.callbacks import setup_all_callbacks, setup_experiment_handlers\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéõÔ∏è INITIALIZING RIS RESEARCH DASHBOARD\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Create dashboard interface\n",
    "print(\"Creating dashboard interface...\")\n",
    "complete_ui, widget_dict = create_complete_interface()\n",
    "\n",
    "# Setup callbacks\n",
    "print(\"Connecting interactive callbacks...\")\n",
    "setup_all_callbacks(widget_dict)\n",
    "\n",
    "# Setup experiment handlers\n",
    "print(\"Connecting experiment handlers...\")\n",
    "setup_experiment_handlers(widget_dict)\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ DASHBOARD READY\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"üìã Quick Tips:\")\n",
    "print(\"   ‚Ä¢ Use tabs to configure parameters\")\n",
    "print(\"   ‚Ä¢ Add experiments to stack with custom names\")\n",
    "print(\"   ‚Ä¢ Transfer learning: Select source experiment\")\n",
    "print(\"   ‚Ä¢ Phase 2: Switch to MATLAB backend in Physics tab\")\n",
    "print()\n",
    "\n",
    "# Display dashboard\n",
    "display(complete_ui)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# DIAGNOSTIC TEST - Run this in a notebook cell\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Simulate your current setup\n",
    "K = 64\n",
    "batch_size = 32\n",
    "\n",
    "# Model output (logits)\n",
    "logits = torch.randn(batch_size, K)\n",
    "\n",
    "# Labels (probe indices)\n",
    "labels = torch.randint(0, K, (batch_size,))\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DIAGNOSTIC TEST: Loss Function Check\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test 1: MSELoss (what you currently have)\n",
    "mse_loss = nn.MSELoss()\n",
    "try:\n",
    "    loss_mse = mse_loss(logits, labels.float().unsqueeze(1))\n",
    "    print(f\"\\n‚úó MSELoss: {loss_mse.item():.4f}\")\n",
    "    print(\"  Problem: Comparing logits to label indices doesn't make sense!\")\n",
    "except:\n",
    "    print(\"\\n‚úó MSELoss: FAILED (dimension mismatch)\")\n",
    "\n",
    "# Test 2: CrossEntropyLoss (what you should have)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "try:\n",
    "    loss_ce = ce_loss(logits, labels)\n",
    "    print(f\"\\n‚úì CrossEntropyLoss: {loss_ce.item():.4f}\")\n",
    "    print(\"  This is correct for classification!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚úó CrossEntropyLoss: FAILED - {e}\")\n",
    "\n",
    "# Test 3: Check if model is actually learning\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING SIMULATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create simple model\n",
    "simple_model = nn.Sequential(\n",
    "    nn.Linear(2*K, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, K)\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(simple_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\nWith CrossEntropyLoss:\")\n",
    "for epoch in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = simple_model(torch.randn(batch_size, 2*K))\n",
    "    loss = ce_loss(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Check accuracy\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    acc = (preds == labels).float().mean().item()\n",
    "\n",
    "    print(f\"  Epoch {epoch+1}: Loss={loss.item():.4f}, Acc={acc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# In a NEW notebook cell (if you can open one) or new notebook entirely:\n",
    "import os\n",
    "import signal\n",
    "\n",
    "# Get current process ID\n",
    "pid = os.getpid()\n",
    "print(f\"Current process: {pid}\")\n",
    "\n",
    "# Force kill\n",
    "os.kill(pid, signal.SIGTERM)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "%cd ..\n",
    "def print_minimal_tree(startpath):\n",
    "    # Redundant folders and session-specific results to skip\n",
    "    exclude_dirs = {'.git', '.idea', '__pycache__', '.venv', 'venv', 'results'}\n",
    "    # File types to hide\n",
    "    exclude_exts = {'.png', '.pt', '.json', '.pkl', '.ipynb_checkpoints'}\n",
    "\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        # Prune redundant directories so the script doesn't even look inside them\n",
    "        dirs[:] = [d for d in dirs if d not in exclude_dirs and not d.startswith('.')]\n",
    "\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = '‚îÇ   ' * level\n",
    "\n",
    "        # Print only the folder name\n",
    "        curr_folder = os.path.basename(root) or os.path.basename(os.getcwd())\n",
    "        print(f\"{indent}‚îú‚îÄ‚îÄ {curr_folder}/\")\n",
    "\n",
    "        # Filter and print main files\n",
    "        sub_indent = '‚îÇ   ' * (level + 1)\n",
    "        main_files = [f for f in files if not f.startswith('.')\n",
    "                      and not any(f.endswith(ext) for ext in exclude_exts)]\n",
    "\n",
    "        for i, f in enumerate(main_files):\n",
    "            connector = '‚îî‚îÄ‚îÄ ' if (i == len(main_files) - 1 and not dirs) else '‚îú‚îÄ‚îÄ '\n",
    "            print(f\"{sub_indent}{connector}{f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print_minimal_tree('.')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "o = F.one_hot(torch.tensor(0), num_classes=3)\n",
    "print(o)\n",
    "\n",
    "scores =torch.tensor([-4.1,5.2,0.7])\n",
    "# one_hot_target =  torch.tensor([1,0,0])\n",
    "one_hot_target =  o\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(criterion(scores.double(), one_hot_target.double()))\n",
    "\n",
    "model = nn.Sequential(nn.Linear(16, 8),\n",
    "                      nn.Linear(8, 4),\n",
    "                      nn.Linear(4, 2))\n",
    "prediction = model()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(op, one_hot_target)\n",
    "loss.backward()\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "weight = model[0].weight\n",
    "weight_grad = model[0].weight.grad\n",
    "\n",
    "weight = weight - lr * weight_grad\n",
    "\n",
    "bias = model[0].bias\n",
    "bias_grad = model[0].bias.grad\n",
    "bias = bias - lr * bias_grad\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "optimizer.step()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:09.782344Z",
     "start_time": "2026-01-21T22:41:07.755974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tests/verify_expected_performance.py\n",
    "import sys\n",
    "import os\n",
    "os.getcwd()\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import numpy as np\n",
    "from data.probe_generators import get_probe_bank\n",
    "from data.data_generation import generate_limited_probing_dataset\n",
    "\n",
    "N, K, M = 32, 64, 8\n",
    "probe_bank = get_probe_bank('continuous', N=N, K=K, seed=42)\n",
    "\n",
    "system_config = {'N': N, 'K': K, 'M': M, 'P_tx': 1.0, 'sigma_h_sq': 1.0, 'sigma_g_sq': 1.0}\n",
    "\n",
    "data = generate_limited_probing_dataset(\n",
    "    probe_bank=probe_bank,\n",
    "    n_samples=1000,\n",
    "    M=M,\n",
    "    system_config=system_config,\n",
    "    normalize=True,\n",
    "    normalization_method='mean_sample',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Check: Is best probe in the M observed probes?\n",
    "best_in_observed = 0\n",
    "for i in range(1000):\n",
    "    best_probe = data['labels'][i]\n",
    "    observed_probes = data['observed_indices'][i]\n",
    "    if best_probe in observed_probes:\n",
    "        best_in_observed += 1\n",
    "\n",
    "print(f\"Best probe in M={M} observed probes: {best_in_observed}/1000 = {best_in_observed/10:.1f}%\")\n",
    "print(f\"Expected if random: M/K = {M}/{K} = {100*M/K:.1f}%\")\n",
    "print(f\"\\nThis is the CEILING for model accuracy - can't predict what you haven't measured!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best probe in M=8 observed probes: 135/1000 = 13.5%\n",
      "Expected if random: M/K = 8/64 = 12.5%\n",
      "\n",
      "This is the CEILING for model accuracy - can't predict what you haven't measured!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# fix_emojis.py\n",
    "import re\n",
    "\n",
    "def remove_emojis_from_file(filepath):\n",
    "    \"\"\"Remove emoji characters from a file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Remove common emojis\n",
    "    emojis = ['üîß', 'üìä', 'üéØ', 'üß†', 'üìà', '‚úÖ', '‚ùå', 'üì°', '‚ö†Ô∏è', 'üî®', '‚úì', '‚èπÔ∏è', '‚ÑπÔ∏è']\n",
    "\n",
    "    for emoji in emojis:\n",
    "        content = content.replace(emoji + ' ', '')\n",
    "        content = content.replace(emoji, '')\n",
    "\n",
    "    # Save with utf-8 encoding\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "    print(f\"Fixed {filepath}\")\n",
    "\n",
    "# Fix both files\n",
    "remove_emojis_from_file('dashboard/experiment_runner.py')\n",
    "remove_emojis_from_file('data/data_generation.py')\n",
    "\n",
    "print(\"Done! Emojis removed from both files.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:42:40.809198Z",
     "start_time": "2026-01-21T22:42:33.136173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tests/proper_evaluation.py\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from data.probe_generators import get_probe_bank\n",
    "from data.data_generation import generate_limited_probing_dataset\n",
    "from models.base_models import BaselineMLPPredictor\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "N, K, M = 32, 64, 8\n",
    "probe_bank = get_probe_bank('continuous', N=N, K=K, seed=42)\n",
    "\n",
    "system_config = {'N': N, 'K': K, 'M': M, 'P_tx': 1.0, 'sigma_h_sq': 1.0, 'sigma_g_sq': 1.0}\n",
    "\n",
    "# Generate test data\n",
    "test_data = generate_limited_probing_dataset(\n",
    "    probe_bank=probe_bank,\n",
    "    n_samples=1000,\n",
    "    M=M,\n",
    "    system_config=system_config,\n",
    "    normalize=True,\n",
    "    normalization_method='mean_sample',\n",
    "    seed=999\n",
    ")\n",
    "\n",
    "# Create trained model\n",
    "train_data = generate_limited_probing_dataset(\n",
    "    probe_bank=probe_bank,\n",
    "    n_samples=10000,\n",
    "    M=M,\n",
    "    system_config=system_config,\n",
    "    normalize=True,\n",
    "    normalization_method='mean_sample',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "train_inputs = torch.cat([\n",
    "    torch.FloatTensor(train_data['masked_powers']),\n",
    "    torch.FloatTensor(train_data['masks'])\n",
    "], dim=1)\n",
    "train_labels = torch.LongTensor(train_data['labels'])\n",
    "train_loader = DataLoader(TensorDataset(train_inputs, train_labels), batch_size=128, shuffle=True)\n",
    "\n",
    "model = BaselineMLPPredictor(2*K, [256, 128], K, 0.1, True)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training model...\")\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(inputs), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate with PROPER metrics\n",
    "model.eval()\n",
    "test_inputs = torch.cat([\n",
    "    torch.FloatTensor(test_data['masked_powers']),\n",
    "    torch.FloatTensor(test_data['masks'])\n",
    "], dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_inputs)\n",
    "    predictions = outputs.numpy()\n",
    "\n",
    "# Metric 1: Top-1 accuracy (what you've been seeing)\n",
    "top1_correct = 0\n",
    "for i in range(len(test_data['labels'])):\n",
    "    pred = np.argmax(predictions[i])\n",
    "    if pred == test_data['labels'][i]:\n",
    "        top1_correct += 1\n",
    "\n",
    "top1_acc = top1_correct / len(test_data['labels'])\n",
    "\n",
    "# Metric 2: Best-of-Observed (much more meaningful!)\n",
    "best_obs_correct = 0\n",
    "for i in range(len(test_data['labels'])):\n",
    "    observed = test_data['observed_indices'][i]\n",
    "    # Only consider predictions for observed probes\n",
    "    observed_scores = predictions[i][observed]\n",
    "    best_observed_idx = observed[np.argmax(observed_scores)]\n",
    "\n",
    "    # Compare to actual best among observed\n",
    "    observed_powers = test_data['powers_full'][i][observed]\n",
    "    true_best_observed = observed[np.argmax(observed_powers)]\n",
    "\n",
    "    if best_observed_idx == true_best_observed:\n",
    "        best_obs_correct += 1\n",
    "\n",
    "best_obs_acc = best_obs_correct / len(test_data['labels'])\n",
    "\n",
    "# Metric 3: Power ratio (eta)\n",
    "etas = []\n",
    "for i in range(len(test_data['labels'])):\n",
    "    observed = test_data['observed_indices'][i]\n",
    "    observed_scores = predictions[i][observed]\n",
    "    selected_probe = observed[np.argmax(observed_scores)]\n",
    "\n",
    "    selected_power = test_data['powers_full'][i][selected_probe]\n",
    "    optimal_power = test_data['optimal_powers'][i]\n",
    "\n",
    "    eta = selected_power / optimal_power\n",
    "    etas.append(eta)\n",
    "\n",
    "mean_eta = np.mean(etas)\n",
    "\n",
    "# Baselines\n",
    "random_in_observed = M / K  # 12.5%\n",
    "random_any = 1 / K  # 1.56%\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROPER EVALUATION METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n1. Top-1 Accuracy (predicting global best from M observations):\")\n",
    "print(f\"   Model: {top1_acc*100:.1f}%\")\n",
    "print(f\"   Theoretical ceiling: {random_in_observed*100:.1f}%\")\n",
    "print(f\"   Random guess: {random_any*100:.1f}%\")\n",
    "print(f\"   Model efficiency: {top1_acc/random_in_observed*100:.1f}% of theoretical max\")\n",
    "\n",
    "print(f\"\\n2. Best-of-Observed Accuracy (picking best among M=8):\")\n",
    "print(f\"   Model: {best_obs_acc*100:.1f}%\")\n",
    "print(f\"   Random: {100/M:.1f}%\")\n",
    "print(f\"   ‚Üí This shows the model IS learning patterns!\")\n",
    "\n",
    "print(f\"\\n3. Power Ratio (Œ∑ - fraction of optimal power):\")\n",
    "print(f\"   Model: {mean_eta:.3f}\")\n",
    "print(f\"   Oracle (perfect): 1.000\")\n",
    "print(f\"   Random M probes: ~0.45\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSION: Your model is working correctly!\")\n",
    "print(\"=\"*70)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "\n",
      "======================================================================\n",
      "PROPER EVALUATION METRICS\n",
      "======================================================================\n",
      "\n",
      "1. Top-1 Accuracy (predicting global best from M observations):\n",
      "   Model: 6.0%\n",
      "   Theoretical ceiling: 12.5%\n",
      "   Random guess: 1.6%\n",
      "   Model efficiency: 48.0% of theoretical max\n",
      "\n",
      "2. Best-of-Observed Accuracy (picking best among M=8):\n",
      "   Model: 55.4%\n",
      "   Random: 12.5%\n",
      "   ‚Üí This shows the model IS learning patterns!\n",
      "\n",
      "3. Power Ratio (Œ∑ - fraction of optimal power):\n",
      "   Model: 0.108\n",
      "   Oracle (perfect): 1.000\n",
      "   Random M probes: ~0.45\n",
      "\n",
      "======================================================================\n",
      "CONCLUSION: Your model is working correctly!\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:43:30.179996Z",
     "start_time": "2026-01-21T22:43:30.050160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tests/diagnose_power_mismatch.py\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import numpy as np\n",
    "from data.probe_generators import get_probe_bank\n",
    "from data.data_generation import generate_limited_probing_dataset\n",
    "\n",
    "N, K, M = 32, 64, 8\n",
    "probe_bank = get_probe_bank('continuous', N=N, K=K, seed=42)\n",
    "system_config = {'N': N, 'K': K, 'M': M, 'P_tx': 1.0, 'sigma_h_sq': 1.0, 'sigma_g_sq': 1.0}\n",
    "\n",
    "# Generate data\n",
    "data = generate_limited_probing_dataset(\n",
    "    probe_bank=probe_bank,\n",
    "    n_samples=1000,\n",
    "    M=M,\n",
    "    system_config=system_config,\n",
    "    normalize=True,\n",
    "    normalization_method='mean_sample',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"POWER DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check the actual power values\n",
    "powers_full = data['powers_full']\n",
    "optimal_powers = data['optimal_powers']\n",
    "\n",
    "print(f\"\\nFull probe powers statistics:\")\n",
    "print(f\"  Mean: {np.mean(powers_full):.4f}\")\n",
    "print(f\"  Std: {np.std(powers_full):.4f}\")\n",
    "print(f\"  Min: {np.min(powers_full):.4f}\")\n",
    "print(f\"  Max: {np.max(powers_full):.4f}\")\n",
    "\n",
    "print(f\"\\nOptimal powers statistics:\")\n",
    "print(f\"  Mean: {np.mean(optimal_powers):.4f}\")\n",
    "print(f\"  Std: {np.std(optimal_powers):.4f}\")\n",
    "print(f\"  Min: {np.min(optimal_powers):.4f}\")\n",
    "print(f\"  Max: {np.max(optimal_powers):.4f}\")\n",
    "\n",
    "# Check ratio of max probe power to optimal\n",
    "ratios = []\n",
    "for i in range(1000):\n",
    "    max_probe_power = np.max(powers_full[i])\n",
    "    optimal = optimal_powers[i]\n",
    "    ratios.append(max_probe_power / optimal)\n",
    "\n",
    "print(f\"\\nRatio of best probe power to optimal power:\")\n",
    "print(f\"  Mean: {np.mean(ratios):.4f}\")\n",
    "print(f\"  Expected: ~0.25-0.30 for random RIS phases\")\n",
    "\n",
    "if np.mean(ratios) < 0.15:\n",
    "    print(f\"  ‚ùå PROBLEM: Ratio is too low!\")\n",
    "    print(f\"     This suggests probes are not achieving good power\")\n",
    "    print(f\"     Possible causes:\")\n",
    "    print(f\"     1. Probe bank phases are not optimized\")\n",
    "    print(f\"     2. Channel generation issue\")\n",
    "elif np.mean(ratios) > 0.9:\n",
    "    print(f\"  ‚ùå PROBLEM: Ratio is too high!\")\n",
    "    print(f\"     This suggests probes are too similar to optimal\")\n",
    "    print(f\"     The task is too easy - no learning needed\")\n",
    "else:\n",
    "    print(f\"  ‚úì Ratio looks reasonable\")\n",
    "\n",
    "# Check if best observed probe achieves reasonable power\n",
    "best_obs_ratios = []\n",
    "for i in range(1000):\n",
    "    observed = data['observed_indices'][i]\n",
    "    observed_powers = powers_full[i][observed]\n",
    "    best_observed_power = np.max(observed_powers)\n",
    "    optimal = optimal_powers[i]\n",
    "    best_obs_ratios.append(best_observed_power / optimal)\n",
    "\n",
    "print(f\"\\nRatio of best OBSERVED probe to optimal:\")\n",
    "print(f\"  Mean: {np.mean(best_obs_ratios):.4f}\")\n",
    "print(f\"  This is what the model should achieve at minimum\")\n",
    "\n",
    "print(\"=\"*70)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "POWER DISTRIBUTION ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Full probe powers statistics:\n",
      "  Mean: 32.2612\n",
      "  Std: 33.1857\n",
      "  Min: 0.0001\n",
      "  Max: 367.3693\n",
      "\n",
      "Optimal powers statistics:\n",
      "  Mean: 645.7880\n",
      "  Std: 182.3412\n",
      "  Min: 219.8416\n",
      "  Max: 1412.2096\n",
      "\n",
      "Ratio of best probe power to optimal power:\n",
      "  Mean: 0.2177\n",
      "  Expected: ~0.25-0.30 for random RIS phases\n",
      "  ‚úì Ratio looks reasonable\n",
      "\n",
      "Ratio of best OBSERVED probe to optimal:\n",
      "  Mean: 0.1311\n",
      "  This is what the model should achieve at minimum\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:44:42.602062Z",
     "start_time": "2026-01-21T22:44:29.033051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tests/test_with_oracle_selection.py\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from data.probe_generators import get_probe_bank\n",
    "from data.data_generation import generate_limited_probing_dataset\n",
    "from models.base_models import BaselineMLPPredictor\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "N, K, M = 32, 64, 8\n",
    "probe_bank = get_probe_bank('continuous', N=N, K=K, seed=42)\n",
    "system_config = {'N': N, 'K': K, 'M': M, 'P_tx': 1.0, 'sigma_h_sq': 1.0, 'sigma_g_sq': 1.0}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARISON: Random vs Oracle Probe Selection\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for selection_method in ['random', 'oracle_topM']:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Testing with: {selection_method}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Generate training data\n",
    "    train_data = generate_limited_probing_dataset(\n",
    "        probe_bank=probe_bank,\n",
    "        n_samples=10000,\n",
    "        M=M,\n",
    "        system_config=system_config,\n",
    "        normalize=True,\n",
    "        normalization_method='mean_sample',\n",
    "        selection_method=selection_method,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Generate test data\n",
    "    test_data = generate_limited_probing_dataset(\n",
    "        probe_bank=probe_bank,\n",
    "        n_samples=1000,\n",
    "        M=M,\n",
    "        system_config=system_config,\n",
    "        normalize=True,\n",
    "        normalization_method='mean_sample',\n",
    "        selection_method=selection_method,\n",
    "        seed=999\n",
    "    )\n",
    "\n",
    "    # Quick check of data quality\n",
    "    best_obs_ratios = []\n",
    "    for i in range(1000):\n",
    "        observed = test_data['observed_indices'][i]\n",
    "        observed_powers = test_data['powers_full'][i][observed]\n",
    "        best_observed_power = np.max(observed_powers)\n",
    "        optimal = test_data['optimal_powers'][i]\n",
    "        best_obs_ratios.append(best_observed_power / optimal)\n",
    "\n",
    "    print(f\"\\nData quality:\")\n",
    "    print(f\"  Best observed / optimal: {np.mean(best_obs_ratios):.4f}\")\n",
    "\n",
    "    # Train model\n",
    "    train_inputs = torch.cat([\n",
    "        torch.FloatTensor(train_data['masked_powers']),\n",
    "        torch.FloatTensor(train_data['masks'])\n",
    "    ], dim=1)\n",
    "    train_labels = torch.LongTensor(train_data['labels'])\n",
    "    train_loader = DataLoader(TensorDataset(train_inputs, train_labels), batch_size=128, shuffle=True)\n",
    "\n",
    "    model = BaselineMLPPredictor(2*K, [256, 128], K, 0.1, True)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(inputs), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    test_inputs = torch.cat([\n",
    "        torch.FloatTensor(test_data['masked_powers']),\n",
    "        torch.FloatTensor(test_data['masks'])\n",
    "    ], dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_inputs)\n",
    "        predictions = outputs.numpy()\n",
    "\n",
    "    # Calculate eta\n",
    "    etas = []\n",
    "    for i in range(len(test_data['labels'])):\n",
    "        observed = test_data['observed_indices'][i]\n",
    "        observed_scores = predictions[i][observed]\n",
    "        selected_probe = observed[np.argmax(observed_scores)]\n",
    "\n",
    "        selected_power = test_data['powers_full'][i][selected_probe]\n",
    "        optimal_power = test_data['optimal_powers'][i]\n",
    "\n",
    "        eta = selected_power / optimal_power\n",
    "        etas.append(eta)\n",
    "\n",
    "    mean_eta = np.mean(etas)\n",
    "\n",
    "    print(f\"\\nModel performance:\")\n",
    "    print(f\"  Œ∑ (power ratio): {mean_eta:.4f}\")\n",
    "    print(f\"  Improvement over random: {mean_eta/0.108:.2f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*70)\n",
    "print(\"If Œ∑ is much higher with oracle_topM, your model works correctly!\")\n",
    "print(\"The issue is just that random probe selection gives poor starting points.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPARISON: Random vs Oracle Probe Selection\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Testing with: random\n",
      "======================================================================\n",
      "\n",
      "Data quality:\n",
      "  Best observed / optimal: 0.1296\n",
      "\n",
      "Model performance:\n",
      "  Œ∑ (power ratio): 0.1077\n",
      "  Improvement over random: 1.00x\n",
      "\n",
      "======================================================================\n",
      "Testing with: oracle_topM\n",
      "======================================================================\n",
      "\n",
      "Data quality:\n",
      "  Best observed / optimal: 0.2177\n",
      "\n",
      "Model performance:\n",
      "  Œ∑ (power ratio): 0.2143\n",
      "  Improvement over random: 1.98x\n",
      "\n",
      "======================================================================\n",
      "CONCLUSION\n",
      "======================================================================\n",
      "If Œ∑ is much higher with oracle_topM, your model works correctly!\n",
      "The issue is just that random probe selection gives poor starting points.\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
