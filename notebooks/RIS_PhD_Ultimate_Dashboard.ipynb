{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì RIS PhD Ultimate Research Dashboard\n",
    "\n",
    "## Comprehensive, Fully Customizable Research Platform\n",
    "\n",
    "**Version:** 1.0.0  \n",
    "**Purpose:** Professional-grade research platform for RIS probe-based ML experiments\n",
    "\n",
    "### Features:\n",
    "- ‚úÖ 5 Customization Tabs (System, Model, Training, Evaluation, Visualization)\n",
    "- ‚úÖ 19 Pre-defined Model Architectures + Custom\n",
    "- ‚úÖ 6 Probe Types (continuous, binary, 2bit, hadamard, sobol, halton)\n",
    "- ‚úÖ 25+ Plot Types\n",
    "- ‚úÖ Multi-Model Comparison\n",
    "- ‚úÖ Multi-Seed Statistical Analysis\n",
    "- ‚úÖ Config Save/Load (JSON/YAML)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup & Installation Check\n",
    "\n",
    "Verify environment setup and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running from repo root\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get notebook directory\n",
    "notebook_dir = Path.cwd()\n",
    "repo_root = notebook_dir.parent if notebook_dir.name == 'notebooks' else notebook_dir\n",
    "\n",
    "# Add repo root to path\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print(f\"üìÅ Notebook directory: {notebook_dir}\")\n",
    "print(f\"üìÅ Repository root: {repo_root}\")\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Change to repo root if needed\n",
    "if os.getcwd() != str(repo_root):\n",
    "    os.chdir(repo_root)\n",
    "    print(f\"‚úÖ Changed working directory to: {os.getcwd()}\")\n",
    "\n",
    "# Verify all dependencies\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DEPENDENCY CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "dependencies = {\n",
    "    'numpy': 'numpy',\n",
    "    'torch': 'torch',\n",
    "    'matplotlib': 'matplotlib',\n",
    "    'seaborn': 'seaborn',\n",
    "    'tqdm': 'tqdm',\n",
    "    'pandas': 'pandas',\n",
    "    'scipy': 'scipy',\n",
    "    'ipywidgets': 'ipywidgets',\n",
    "    'yaml': 'pyyaml',\n",
    "}\n",
    "\n",
    "missing = []\n",
    "for module, package in dependencies.items():\n",
    "    try:\n",
    "        __import__(module)\n",
    "        print(f\"‚úÖ {package:15s} installed\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {package:15s} NOT FOUND\")\n",
    "        missing.append(package)\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\n‚ö†Ô∏è  Missing packages: {', '.join(missing)}\")\n",
    "    print(\"\\nInstall with: pip install \" + \" \".join(missing))\n",
    "else:\n",
    "    print(\"\\n‚úÖ All dependencies installed!\")\n",
    "\n",
    "# System info\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SYSTEM INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Python version:  {sys.version.split()[0]}\")\n",
    "print(f\"NumPy version:   {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available:  {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version:    {torch.version.cuda}\")\n",
    "    print(f\"GPU device:      {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Import Dashboard Components\n",
    "\n",
    "Load the dashboard system and initialize widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dashboard components\n",
    "import dashboard\n",
    "from dashboard import (\n",
    "    get_all_widgets,\n",
    "    create_tab_layout,\n",
    "    setup_all_callbacks,\n",
    "    reset_to_defaults,\n",
    "    get_validation_errors,\n",
    "    config_to_dict,\n",
    "    dict_to_widgets,\n",
    "    save_config,\n",
    "    load_config,\n",
    "    run_single_experiment,\n",
    "    run_multi_model_comparison,\n",
    "    run_multi_seed_experiment,\n",
    "    aggregate_results,\n",
    ")\n",
    "\n",
    "# Import plotting\n",
    "from dashboard.plots import EXTENDED_PLOT_REGISTRY\n",
    "\n",
    "# Import ipywidgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Print welcome\n",
    "dashboard.print_welcome()\n",
    "\n",
    "# Initialize widgets\n",
    "print(\"Initializing widget system...\")\n",
    "widgets_dict = get_all_widgets()\n",
    "setup_all_callbacks(widgets_dict)\n",
    "print(\"‚úÖ Widget system initialized!\")\n",
    "print(f\"‚úÖ {len(EXTENDED_PLOT_REGISTRY)} plot types available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cells 3-7: Widget-Based Control Panel (5 Tabs)\n",
    "\n",
    "### Comprehensive Parameter Configuration\n",
    "\n",
    "Use the tabs below to configure all aspects of your experiment:\n",
    "\n",
    "1. **‚öôÔ∏è System & Physics** - RIS elements, codebook size, sensing budget, channel parameters, probe types\n",
    "2. **üß† Model Architecture** - Pre-defined models or custom architecture, dropout, activations\n",
    "3. **üìä Training Config** - Dataset sizes, batch size, learning rate, optimizer, scheduler\n",
    "4. **üìà Evaluation** - Top-m values, model comparison, multi-seed runs\n",
    "5. **üé® Visualization** - Plot selection, output format, styling\n",
    "\n",
    "**üí° Tip:** Hover over widgets for descriptions. Changes are validated automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display tabbed interface\n",
    "tabs = create_tab_layout()\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Reference Table\n",
    "\n",
    "| Category | Parameter | Default | Range | Description |\n",
    "|----------|-----------|---------|-------|-------------|\n",
    "| **System** | N | 32 | 4-256 | Number of RIS elements |\n",
    "| | K | 64 | 4-512 | Total probes in codebook |\n",
    "| | M | 8 | 1-K | Sensing budget (probes measured) |\n",
    "| | P_tx | 1.0 | 0.1-10 | Transmit power |\n",
    "| | probe_type | continuous | 6 options | Probe generation method |\n",
    "| **Model** | hidden_sizes | [512,256,128] | varies | Layer architecture |\n",
    "| | dropout_prob | 0.1 | 0-0.8 | Dropout regularization |\n",
    "| **Training** | n_train | 50000 | 1000+ | Training samples |\n",
    "| | learning_rate | 1e-3 | 1e-5 to 1e-1 | Learning rate |\n",
    "| | batch_size | 128 | 32-512 | Batch size |\n",
    "| | n_epochs | 50 | 1-500 | Maximum epochs |\n",
    "| **Eval** | top_m_values | [1,2,4,8] | 1-K | Top-m accuracy metrics |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Action Buttons & Status\n",
    "\n",
    "Control experiment execution and configuration management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for experiment state\n",
    "current_results = None\n",
    "current_config = None\n",
    "\n",
    "# Define button callbacks\n",
    "def on_run_experiment_clicked(b):\n",
    "    \"\"\"Execute experiment with current configuration.\"\"\"\n",
    "    global current_results, current_config\n",
    "    \n",
    "    # Clear previous output\n",
    "    widgets_dict['status_output'].clear_output()\n",
    "    widgets_dict['results_summary'].value = \"<div style='padding:10px;'><i>Running experiment...</i></div>\"\n",
    "    widgets_dict['results_plots'].clear_output()\n",
    "    \n",
    "    with widgets_dict['status_output']:\n",
    "        try:\n",
    "            print(\"üöÄ Starting experiment...\\n\")\n",
    "            \n",
    "            # Get configuration from widgets\n",
    "            config_dict = config_to_dict(widgets_dict)\n",
    "            current_config = config_dict\n",
    "            \n",
    "            # Validate configuration\n",
    "            is_valid, errors = get_validation_errors(config_dict)\n",
    "            if not is_valid:\n",
    "                print(\"‚ùå Configuration validation failed:\\n\")\n",
    "                for error in errors:\n",
    "                    print(f\"  - {error}\")\n",
    "                return\n",
    "            \n",
    "            print(\"‚úÖ Configuration validated\\n\")\n",
    "            \n",
    "            # Progress callback\n",
    "            def progress_callback(epoch, total_epochs, metrics):\n",
    "                progress = int((epoch / total_epochs) * 100)\n",
    "                widgets_dict['progress_bar'].value = progress\n",
    "                \n",
    "                metrics_html = f\"\"\"<div style='font-family: monospace; padding: 10px;'>\n",
    "                <b>Epoch {epoch}/{total_epochs}</b><br>\n",
    "                Train Loss: {metrics.get('train_loss', 0):.4f}<br>\n",
    "                Val Loss: {metrics.get('val_loss', 0):.4f}<br>\n",
    "                Val Accuracy: {metrics.get('val_acc', 0):.3f}<br>\n",
    "                Val Œ∑: {metrics.get('val_eta', 0):.4f}<br>\n",
    "                </div>\"\"\"\n",
    "                widgets_dict['live_metrics'].value = metrics_html\n",
    "            \n",
    "            # Check if multi-model comparison\n",
    "            if config_dict.get('compare_multiple_models', False):\n",
    "                models_to_compare = list(config_dict.get('models_to_compare', []))\n",
    "                if len(models_to_compare) >= 2:\n",
    "                    print(f\"Running multi-model comparison: {len(models_to_compare)} models\\n\")\n",
    "                    current_results = run_multi_model_comparison(\n",
    "                        base_config_dict=config_dict,\n",
    "                        model_names=models_to_compare,\n",
    "                        progress_callback=progress_callback,\n",
    "                        verbose=True\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è Need at least 2 models for comparison. Running single experiment.\\n\")\n",
    "                    current_results = run_single_experiment(\n",
    "                        config_dict=config_dict,\n",
    "                        progress_callback=progress_callback,\n",
    "                        verbose=True\n",
    "                    )\n",
    "            # Check if multi-seed runs\n",
    "            elif config_dict.get('multi_seed_runs', False):\n",
    "                num_seeds = config_dict.get('num_seeds', 3)\n",
    "                base_seed = config_dict.get('seed', 42)\n",
    "                seeds = [base_seed + i for i in range(num_seeds)]\n",
    "                \n",
    "                print(f\"Running multi-seed experiment: {num_seeds} seeds\\n\")\n",
    "                current_results = run_multi_seed_experiment(\n",
    "                    config_dict=config_dict,\n",
    "                    seeds=seeds,\n",
    "                    progress_callback=progress_callback,\n",
    "                    verbose=True\n",
    "                )\n",
    "            else:\n",
    "                # Single experiment\n",
    "                current_results = run_single_experiment(\n",
    "                    config_dict=config_dict,\n",
    "                    progress_callback=progress_callback,\n",
    "                    verbose=True\n",
    "                )\n",
    "            \n",
    "            print(\"\\n‚úÖ Experiment completed successfully!\")\n",
    "            widgets_dict['progress_bar'].value = 100\n",
    "            \n",
    "            # Update results display\n",
    "            update_results_display()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Experiment failed with error:\\n{str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "def on_save_config_clicked(b):\n",
    "    \"\"\"Save current configuration to file.\"\"\"\n",
    "    widgets_dict['status_output'].clear_output()\n",
    "    \n",
    "    with widgets_dict['status_output']:\n",
    "        try:\n",
    "            config_dict = config_to_dict(widgets_dict)\n",
    "            \n",
    "            # Create configs directory if needed\n",
    "            os.makedirs('configs', exist_ok=True)\n",
    "            \n",
    "            # Save as both JSON and YAML\n",
    "            import time\n",
    "            timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "            json_path = f\"configs/config_{timestamp}.json\"\n",
    "            yaml_path = f\"configs/config_{timestamp}.yaml\"\n",
    "            \n",
    "            if save_config(config_dict, json_path):\n",
    "                print(f\"‚úÖ Configuration saved to: {json_path}\")\n",
    "            \n",
    "            if save_config(config_dict, yaml_path):\n",
    "                print(f\"‚úÖ Configuration saved to: {yaml_path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to save configuration: {str(e)}\")\n",
    "\n",
    "\n",
    "def on_load_config_clicked(b):\n",
    "    \"\"\"Load configuration from file.\"\"\"\n",
    "    widgets_dict['status_output'].clear_output()\n",
    "    \n",
    "    with widgets_dict['status_output']:\n",
    "        try:\n",
    "            # List available configs\n",
    "            if os.path.exists('configs'):\n",
    "                files = sorted([f for f in os.listdir('configs') \n",
    "                              if f.endswith(('.json', '.yaml', '.yml'))])\n",
    "                \n",
    "                if files:\n",
    "                    print(\"Available configuration files:\")\n",
    "                    for i, f in enumerate(files, 1):\n",
    "                        print(f\"  {i}. {f}\")\n",
    "                    print(\"\\nTo load a config, use: dict_to_widgets(load_config('configs/filename.json'), widgets_dict)\")\n",
    "                else:\n",
    "                    print(\"No saved configurations found in 'configs/' directory.\")\n",
    "            else:\n",
    "                print(\"No 'configs/' directory found. Save a configuration first.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load configuration: {str(e)}\")\n",
    "\n",
    "\n",
    "def on_reset_defaults_clicked(b):\n",
    "    \"\"\"Reset all widgets to default values.\"\"\"\n",
    "    widgets_dict['status_output'].clear_output()\n",
    "    \n",
    "    with widgets_dict['status_output']:\n",
    "        print(\"Resetting all parameters to defaults...\")\n",
    "        reset_to_defaults(widgets_dict)\n",
    "        print(\"\\n‚úÖ All parameters reset to defaults!\")\n",
    "\n",
    "\n",
    "def update_results_display():\n",
    "    \"\"\"Update the results display with experiment results.\"\"\"\n",
    "    global current_results\n",
    "    \n",
    "    if current_results is None:\n",
    "        return\n",
    "    \n",
    "    # Generate summary\n",
    "    if isinstance(current_results, dict):  # Multi-model comparison\n",
    "        summary_html = \"<div style='padding: 10px; font-family: sans-serif;'>\"\n",
    "        summary_html += \"<h3>Multi-Model Comparison Results</h3>\"\n",
    "        summary_html += \"<table style='border-collapse: collapse; width: 100%;'>\"\n",
    "        summary_html += \"<tr style='background-color: #f0f0f0;'>\"\n",
    "        summary_html += \"<th style='border: 1px solid #ddd; padding: 8px;'>Model</th>\"\n",
    "        summary_html += \"<th style='border: 1px solid #ddd; padding: 8px;'>Œ∑_top1</th>\"\n",
    "        summary_html += \"<th style='border: 1px solid #ddd; padding: 8px;'>Accuracy</th>\"\n",
    "        summary_html += \"<th style='border: 1px solid #ddd; padding: 8px;'>Time (s)</th>\"\n",
    "        summary_html += \"</tr>\"\n",
    "        \n",
    "        for model_name, result in current_results.items():\n",
    "            summary_html += \"<tr>\"\n",
    "            summary_html += f\"<td style='border: 1px solid #ddd; padding: 8px;'>{model_name}</td>\"\n",
    "            summary_html += f\"<td style='border: 1px solid #ddd; padding: 8px;'>{result.evaluation.eta_top1:.4f}</td>\"\n",
    "            summary_html += f\"<td style='border: 1px solid #ddd; padding: 8px;'>{result.evaluation.accuracy_top1:.3f}</td>\"\n",
    "            summary_html += f\"<td style='border: 1px solid #ddd; padding: 8px;'>{result.execution_time:.1f}</td>\"\n",
    "            summary_html += \"</tr>\"\n",
    "        \n",
    "        summary_html += \"</table></div>\"\n",
    "        \n",
    "    elif isinstance(current_results, list):  # Multi-seed\n",
    "        agg = aggregate_results(current_results)\n",
    "        summary_html = \"<div style='padding: 10px; font-family: sans-serif;'>\"\n",
    "        summary_html += \"<h3>Multi-Seed Experiment Results</h3>\"\n",
    "        summary_html += f\"<p><b>Number of runs:</b> {agg['n_runs']}</p>\"\n",
    "        summary_html += f\"<p><b>Œ∑_top1:</b> {agg['eta_top1']['mean']:.4f} ¬± {agg['eta_top1']['std']:.4f}</p>\"\n",
    "        summary_html += f\"<p><b>Accuracy:</b> {agg['accuracy_top1']['mean']:.3f} ¬± {agg['accuracy_top1']['std']:.3f}</p>\"\n",
    "        summary_html += \"</div>\"\n",
    "        \n",
    "    else:  # Single experiment\n",
    "        result = current_results\n",
    "        summary_html = \"<div style='padding: 10px; font-family: sans-serif;'>\"\n",
    "        summary_html += \"<h3>Experiment Results</h3>\"\n",
    "        summary_html += f\"<p><b>Œ∑_top1:</b> {result.evaluation.eta_top1:.4f}</p>\"\n",
    "        summary_html += f\"<p><b>Accuracy (Top-1):</b> {result.evaluation.accuracy_top1:.3f}</p>\"\n",
    "        summary_html += f\"<p><b>Accuracy (Top-4):</b> {result.evaluation.accuracy_top4:.3f}</p>\"\n",
    "        summary_html += f\"<p><b>Execution time:</b> {result.execution_time:.1f}s</p>\"\n",
    "        summary_html += \"</div>\"\n",
    "    \n",
    "    widgets_dict['results_summary'].value = summary_html\n",
    "    \n",
    "    # Generate plots\n",
    "    generate_plots()\n",
    "\n",
    "\n",
    "def generate_plots():\n",
    "    \"\"\"Generate selected plots for current results.\"\"\"\n",
    "    global current_results, current_config\n",
    "    \n",
    "    if current_results is None:\n",
    "        return\n",
    "    \n",
    "    widgets_dict['results_plots'].clear_output()\n",
    "    \n",
    "    with widgets_dict['results_plots']:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        selected_plots = list(widgets_dict['selected_plots'].value)\n",
    "        dpi = widgets_dict['dpi'].value\n",
    "        color_palette = widgets_dict['color_palette'].value\n",
    "        \n",
    "        print(f\"Generating {len(selected_plots)} plots...\\n\")\n",
    "        \n",
    "        # Set style\n",
    "        from dashboard.plots import set_plot_style\n",
    "        set_plot_style(color_palette)\n",
    "        \n",
    "        for plot_name in selected_plots:\n",
    "            try:\n",
    "                if plot_name in EXTENDED_PLOT_REGISTRY:\n",
    "                    plot_func = EXTENDED_PLOT_REGISTRY[plot_name]\n",
    "                    \n",
    "                    # Call appropriate plot function\n",
    "                    if isinstance(current_results, dict):  # Multi-model\n",
    "                        if plot_name in ['violin', 'box', 'radar_chart', 'model_size_vs_performance', 'pareto_front']:\n",
    "                            plot_func(current_results, dpi=dpi)\n",
    "                        else:\n",
    "                            # Plot for first model\n",
    "                            first_result = list(current_results.values())[0]\n",
    "                            if plot_name in ['training_curves', 'learning_curve', 'convergence_analysis']:\n",
    "                                plot_func(first_result.training_history, dpi=dpi)\n",
    "                            else:\n",
    "                                plot_func(first_result.evaluation, dpi=dpi)\n",
    "                    \n",
    "                    elif isinstance(current_results, list):  # Multi-seed\n",
    "                        # Use first result\n",
    "                        if plot_name in ['training_curves', 'learning_curve']:\n",
    "                            plot_func(current_results[0].training_history, dpi=dpi)\n",
    "                        else:\n",
    "                            plot_func(current_results[0].evaluation, dpi=dpi)\n",
    "                    \n",
    "                    else:  # Single experiment\n",
    "                        if plot_name in ['training_curves', 'learning_curve']:\n",
    "                            plot_func(current_results.training_history, dpi=dpi)\n",
    "                        else:\n",
    "                            plot_func(current_results.evaluation, dpi=dpi)\n",
    "                    \n",
    "                    plt.show()\n",
    "                    print(f\"‚úÖ {plot_name}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Failed to generate {plot_name}: {str(e)}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Plot generation complete!\")\n",
    "\n",
    "\n",
    "# Attach button callbacks\n",
    "widgets_dict['button_run_experiment'].on_click(on_run_experiment_clicked)\n",
    "widgets_dict['button_save_config'].on_click(on_save_config_clicked)\n",
    "widgets_dict['button_load_config'].on_click(on_load_config_clicked)\n",
    "widgets_dict['button_reset_defaults'].on_click(on_reset_defaults_clicked)\n",
    "\n",
    "# Display buttons\n",
    "buttons_box = widgets.HBox([\n",
    "    widgets_dict['button_run_experiment'],\n",
    "    widgets_dict['button_save_config'],\n",
    "    widgets_dict['button_load_config'],\n",
    "    widgets_dict['button_reset_defaults'],\n",
    "], layout=widgets.Layout(justify_content='space-around', padding='20px'))\n",
    "\n",
    "display(buttons_box)\n",
    "\n",
    "# Display status output\n",
    "display(widgets.HTML(\"<h3>Status Output</h3>\"))\n",
    "display(widgets_dict['status_output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Real-Time Progress Display\n",
    "\n",
    "Monitor experiment progress during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display progress widgets\n",
    "display(widgets.HTML(\"<h3>Training Progress</h3>\"))\n",
    "display(widgets_dict['progress_bar'])\n",
    "display(widgets_dict['live_metrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Results Dashboard\n",
    "\n",
    "View experiment results, statistics, and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "display(widgets.HTML(\"<h3>Results Summary</h3>\"))\n",
    "display(widgets_dict['results_summary'])\n",
    "\n",
    "display(widgets.HTML(\"<h3>Visualizations</h3>\"))\n",
    "display(widgets_dict['results_plots'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Export Results\n",
    "\n",
    "Export results to various formats for publication and further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export functions\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def export_results_to_csv(filename='results/experiment_results.csv'):\n",
    "    \"\"\"Export results to CSV.\"\"\"\n",
    "    global current_results\n",
    "    \n",
    "    if current_results is None:\n",
    "        print(\"‚ö†Ô∏è No results to export. Run an experiment first.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        \n",
    "        if isinstance(current_results, dict):  # Multi-model\n",
    "            data = []\n",
    "            for model_name, result in current_results.items():\n",
    "                row = {\n",
    "                    'model': model_name,\n",
    "                    'eta_top1': result.evaluation.eta_top1,\n",
    "                    'accuracy_top1': result.evaluation.accuracy_top1,\n",
    "                    'accuracy_top4': result.evaluation.accuracy_top4,\n",
    "                    'eta_random_1': result.evaluation.eta_random_1,\n",
    "                    'execution_time': result.execution_time,\n",
    "                }\n",
    "                data.append(row)\n",
    "            \n",
    "            df = pd.DataFrame(data)\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"‚úÖ Results exported to: {filename}\")\n",
    "            \n",
    "        else:  # Single or multi-seed\n",
    "            if isinstance(current_results, list):\n",
    "                result = current_results[0]\n",
    "            else:\n",
    "                result = current_results\n",
    "            \n",
    "            data = {\n",
    "                'metric': ['eta_top1', 'eta_top2', 'eta_top4', 'eta_top8',\n",
    "                          'accuracy_top1', 'accuracy_top2', 'accuracy_top4', 'accuracy_top8'],\n",
    "                'value': [\n",
    "                    result.evaluation.eta_top1,\n",
    "                    result.evaluation.eta_top2,\n",
    "                    result.evaluation.eta_top4,\n",
    "                    result.evaluation.eta_top8,\n",
    "                    result.evaluation.accuracy_top1,\n",
    "                    result.evaluation.accuracy_top2,\n",
    "                    result.evaluation.accuracy_top4,\n",
    "                    result.evaluation.accuracy_top8,\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            df = pd.DataFrame(data)\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"‚úÖ Results exported to: {filename}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Export failed: {str(e)}\")\n",
    "\n",
    "\n",
    "def export_results_to_json(filename='results/experiment_results.json'):\n",
    "    \"\"\"Export results to JSON.\"\"\"\n",
    "    global current_results\n",
    "    \n",
    "    if current_results is None:\n",
    "        print(\"‚ö†Ô∏è No results to export. Run an experiment first.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        \n",
    "        if isinstance(current_results, dict):  # Multi-model\n",
    "            export_data = {}\n",
    "            for model_name, result in current_results.items():\n",
    "                export_data[model_name] = result.to_dict()\n",
    "        elif isinstance(current_results, list):  # Multi-seed\n",
    "            export_data = {\n",
    "                'results': [r.to_dict() for r in current_results],\n",
    "                'aggregated': aggregate_results(current_results)\n",
    "            }\n",
    "        else:  # Single\n",
    "            export_data = current_results.to_dict()\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(export_data, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"‚úÖ Results exported to: {filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Export failed: {str(e)}\")\n",
    "\n",
    "\n",
    "def save_trained_model(filename='results/trained_model.pth'):\n",
    "    \"\"\"Save trained model.\"\"\"\n",
    "    global current_results\n",
    "    \n",
    "    if current_results is None:\n",
    "        print(\"‚ö†Ô∏è No model to save. Run an experiment first.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        \n",
    "        if isinstance(current_results, dict):\n",
    "            result = list(current_results.values())[0]\n",
    "        elif isinstance(current_results, list):\n",
    "            result = current_results[0]\n",
    "        else:\n",
    "            result = current_results\n",
    "        \n",
    "        if result.model_state:\n",
    "            torch.save(result.model_state, filename)\n",
    "            print(f\"‚úÖ Model saved to: {filename}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No model state available.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Save failed: {str(e)}\")\n",
    "\n",
    "\n",
    "def generate_latex_table():\n",
    "    \"\"\"Generate LaTeX table for publications.\"\"\"\n",
    "    global current_results\n",
    "    \n",
    "    if current_results is None:\n",
    "        print(\"‚ö†Ô∏è No results to export. Run an experiment first.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        print(\"\\\\begin{table}[htbp]\")\n",
    "        print(\"\\\\centering\")\n",
    "        print(\"\\\\caption{Experiment Results}\")\n",
    "        print(\"\\\\label{tab:results}\")\n",
    "        \n",
    "        if isinstance(current_results, dict):  # Multi-model\n",
    "            print(\"\\\\begin{tabular}{lccc}\")\n",
    "            print(\"\\\\toprule\")\n",
    "            print(\"Model & $\\\\eta_{\\\\text{top-1}}$ & Accuracy & Time (s) \\\\\\\\\")\n",
    "            print(\"\\\\midrule\")\n",
    "            \n",
    "            for model_name, result in current_results.items():\n",
    "                print(f\"{model_name} & {result.evaluation.eta_top1:.4f} & \"\n",
    "                      f\"{result.evaluation.accuracy_top1:.3f} & {result.execution_time:.1f} \\\\\\\\\")\n",
    "            \n",
    "            print(\"\\\\bottomrule\")\n",
    "            print(\"\\\\end{tabular}\")\n",
    "        else:\n",
    "            if isinstance(current_results, list):\n",
    "                result = current_results[0]\n",
    "            else:\n",
    "                result = current_results\n",
    "            \n",
    "            print(\"\\\\begin{tabular}{lc}\")\n",
    "            print(\"\\\\toprule\")\n",
    "            print(\"Metric & Value \\\\\\\\\")\n",
    "            print(\"\\\\midrule\")\n",
    "            print(f\"$\\\\eta_{{\\\\text{{top-1}}}}$ & {result.evaluation.eta_top1:.4f} \\\\\\\\\")\n",
    "            print(f\"$\\\\eta_{{\\\\text{{top-4}}}}$ & {result.evaluation.eta_top4:.4f} \\\\\\\\\")\n",
    "            print(f\"Accuracy (Top-1) & {result.evaluation.accuracy_top1:.3f} \\\\\\\\\")\n",
    "            print(f\"Accuracy (Top-4) & {result.evaluation.accuracy_top4:.3f} \\\\\\\\\")\n",
    "            print(\"\\\\bottomrule\")\n",
    "            print(\"\\\\end{tabular}\")\n",
    "        \n",
    "        print(\"\\\\end{table}\")\n",
    "        print(\"\\n‚úÖ LaTeX table generated!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Generation failed: {str(e)}\")\n",
    "\n",
    "\n",
    "# Create export buttons\n",
    "btn_export_csv = widgets.Button(description=\"Export to CSV\", button_style='info')\n",
    "btn_export_json = widgets.Button(description=\"Export to JSON\", button_style='info')\n",
    "btn_save_model = widgets.Button(description=\"Save Model\", button_style='success')\n",
    "btn_latex_table = widgets.Button(description=\"Generate LaTeX\", button_style='warning')\n",
    "\n",
    "btn_export_csv.on_click(lambda b: export_results_to_csv())\n",
    "btn_export_json.on_click(lambda b: export_results_to_json())\n",
    "btn_save_model.on_click(lambda b: save_trained_model())\n",
    "btn_latex_table.on_click(lambda b: generate_latex_table())\n",
    "\n",
    "export_buttons = widgets.HBox([btn_export_csv, btn_export_json, btn_save_model, btn_latex_table],\n",
    "                              layout=widgets.Layout(justify_content='space-around', padding='20px'))\n",
    "\n",
    "display(widgets.HTML(\"<h3>Export Options</h3>\"))\n",
    "display(export_buttons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Learning Guide\n",
    "\n",
    "### Probe Types Explained\n",
    "\n",
    "1. **Continuous**: Random phases in [0, 2œÄ). Best for theoretical studies.\n",
    "2. **Binary**: Phases {0, œÄ}. Simplest hardware implementation.\n",
    "3. **2-bit**: Phases {0, œÄ/2, œÄ, 3œÄ/2}. Good balance of performance and simplicity.\n",
    "4. **Hadamard**: Structured orthogonal patterns. Excellent diversity.\n",
    "5. **Sobol**: Low-discrepancy quasi-random. Better coverage than random.\n",
    "6. **Halton**: Another quasi-random sequence. Similar to Sobol.\n",
    "\n",
    "### Model Architecture Guidelines\n",
    "\n",
    "- **Wider networks** (e.g., DoubleWide): More capacity, risk of overfitting\n",
    "- **Deeper networks** (e.g., VeryDeep): Better feature extraction, harder to train\n",
    "- **Pyramidal** (e.g., Pyramid): Natural information compression\n",
    "- **Hourglass**: Information bottleneck for robust features\n",
    "- **ResNet-style**: Same width, easier gradient flow\n",
    "\n",
    "### Key Parameter Interactions\n",
    "\n",
    "- **M/K ratio**: Critical for performance. Lower ratio = harder problem.\n",
    "- **Learning rate**: Most important hyperparameter. Start with 1e-3.\n",
    "- **Dropout**: Use 0.1-0.2 for regularization. Higher values for larger models.\n",
    "- **Batch size**: Larger = more stable gradients. Smaller = better generalization.\n",
    "\n",
    "### Typical Workflows\n",
    "\n",
    "1. **Quick Test**: Default settings, 1 epoch, check if system works\n",
    "2. **Architecture Search**: Compare multiple models, same data/training\n",
    "3. **Hyperparameter Tuning**: Fix architecture, sweep learning rates\n",
    "4. **Statistical Validation**: Multi-seed runs for confidence intervals\n",
    "5. **Publication Results**: Best config, full training, all plots\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Troubleshooting\n",
    "\n",
    "**Out of memory?**\n",
    "- Reduce batch_size\n",
    "- Use smaller model\n",
    "- Reduce n_train\n",
    "\n",
    "**Training too slow?**\n",
    "- Reduce n_epochs\n",
    "- Use smaller dataset\n",
    "- Use simpler model\n",
    "\n",
    "**Poor performance?**\n",
    "- Increase model capacity\n",
    "- Try different probe types\n",
    "- Adjust learning rate\n",
    "- More training data\n",
    "\n",
    "**Overfitting?**\n",
    "- Increase dropout\n",
    "- Add weight_decay\n",
    "- Reduce model size\n",
    "- More training data\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ References\n",
    "\n",
    "For more information, see:\n",
    "- `dashboard/README.md` - Detailed documentation\n",
    "- `EXTENSION_GUIDE.md` - How to extend the system\n",
    "- `USAGE_EXAMPLES.md` - Usage examples\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Researching! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
