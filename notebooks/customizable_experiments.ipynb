{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visual Experiment Studio\n",
        "\n",
        "This notebook provides a **visual, organized UI** for controlling **all experiment parameters**.\n",
        "\n",
        "**What you can customize:**\n",
        "- Phase mode (continuous/discrete) and discrete bits\n",
        "- Probe bank method: random, Hadamard, Sobol, or Halton\n",
        "- All system/data/model/training/eval parameters\n",
        "- Which models to compare\n",
        "- Which plots to render\n",
        "\n",
        "> Edit values in the widgets, then press **Run Experiments**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## How to see the Visual IDE\n",
        "\n",
        "1. **Run the first two code cells** (imports + widget UI). The tabs and controls will appear right below the \"Run Experiments\" button.\n",
        "2. If you are in **JupyterLab**, ensure the `ipywidgets` extension is enabled (installed via `requirements.txt`).\n",
        "3. The UI is organized in tabs: **System**, **Data**, **Model**, **Training**, **Eval**, **Compare/Plots**.\n",
        "\n",
        "> If you only see code, it means the widget cell hasn\u2019t been executed yet.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "from config import get_config\n",
        "from data_generation import generate_probe_bank, create_dataloaders\n",
        "from training import train\n",
        "from evaluation import evaluate_model\n",
        "from utils import (\n",
        "    plot_training_history,\n",
        "    plot_eta_distribution,\n",
        "    plot_top_m_comparison,\n",
        "    plot_baseline_comparison,\n",
        ")\n",
        "from model import LimitedProbingMLP\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- MODEL REGISTRY ----\n",
        "def build_mlp_default(config):\n",
        "    return LimitedProbingMLP(\n",
        "        K=config.system.K,\n",
        "        hidden_sizes=config.model.hidden_sizes,\n",
        "        dropout_prob=config.model.dropout_prob,\n",
        "        use_batch_norm=config.model.use_batch_norm,\n",
        "    )\n",
        "\n",
        "def build_mlp_shallow(config):\n",
        "    return LimitedProbingMLP(\n",
        "        K=config.system.K,\n",
        "        hidden_sizes=[128],\n",
        "        dropout_prob=0.0,\n",
        "        use_batch_norm=False,\n",
        "    )\n",
        "\n",
        "def build_linear(config):\n",
        "    input_size = 2 * config.system.K\n",
        "    output_size = config.system.K\n",
        "    return torch.nn.Sequential(torch.nn.Linear(input_size, output_size))\n",
        "\n",
        "MODEL_REGISTRY = {\n",
        "    'mlp_default': build_mlp_default,\n",
        "    'mlp_shallow': build_mlp_shallow,\n",
        "    'linear': build_linear,\n",
        "}\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- PLOT REGISTRY ----\n",
        "def plot_model_metric_bars(runs, metrics):\n",
        "    names = [r['name'] for r in runs]\n",
        "    x = np.arange(len(names))\n",
        "    width = 0.2\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    for i, metric in enumerate(metrics):\n",
        "        values = [getattr(r['results'], metric) for r in runs]\n",
        "        ax.bar(x + i * width, values, width, label=metric)\n",
        "    ax.set_xticks(x + width * (len(metrics) - 1) / 2)\n",
        "    ax.set_xticklabels(names, rotation=15)\n",
        "    ax.set_ylabel('Metric value')\n",
        "    ax.set_title('Model Comparison')\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_eta_boxplot(runs):\n",
        "    labels = [r['name'] for r in runs]\n",
        "    data = [r['results'].eta_top1_distribution for r in runs]\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    ax.boxplot(data, labels=labels, showmeans=True)\n",
        "    ax.set_ylabel('\u03b7 (Top-1)')\n",
        "    ax.set_title('\u03b7 Distribution by Model')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "PLOT_REGISTRY = {\n",
        "    'training_history': lambda run: plot_training_history(run['history']),\n",
        "    'eta_distribution': lambda run: plot_eta_distribution(run['results']),\n",
        "    'top_m_comparison': lambda run: plot_top_m_comparison(run['results']),\n",
        "    'baseline_comparison': lambda run: plot_baseline_comparison(run['results']),\n",
        "    'model_metric_bars': lambda runs, metrics: plot_model_metric_bars(runs, metrics),\n",
        "    'eta_boxplot': lambda runs, _metrics: plot_eta_boxplot(runs),\n",
        "}\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- WIDGET UI ----\n",
        "default_config = get_config()\n",
        "\n",
        "system_widgets = {\n",
        "    'N': widgets.IntText(value=default_config.system.N, description='N'),\n",
        "    'K': widgets.IntText(value=default_config.system.K, description='K'),\n",
        "    'M': widgets.IntText(value=default_config.system.M, description='M'),\n",
        "    'P_tx': widgets.FloatText(value=default_config.system.P_tx, description='P_tx'),\n",
        "    'sigma_h_sq': widgets.FloatText(value=default_config.system.sigma_h_sq, description='sigma_h_sq'),\n",
        "    'sigma_g_sq': widgets.FloatText(value=default_config.system.sigma_g_sq, description='sigma_g_sq'),\n",
        "    'phase_mode': widgets.Dropdown(\n",
        "        options=['continuous', 'discrete'],\n",
        "        value=default_config.system.phase_mode,\n",
        "        description='phase_mode'\n",
        "    ),\n",
        "    'phase_bits': widgets.IntText(value=default_config.system.phase_bits, description='phase_bits'),\n",
        "    'probe_bank_method': widgets.Dropdown(\n",
        "        options=['random', 'hadamard', 'sobol', 'halton'],\n",
        "        value=default_config.system.probe_bank_method,\n",
        "        description='probe_method'\n",
        "    ),\n",
        "}\n",
        "\n",
        "data_widgets = {\n",
        "    'n_train': widgets.IntText(value=default_config.data.n_train, description='n_train'),\n",
        "    'n_val': widgets.IntText(value=default_config.data.n_val, description='n_val'),\n",
        "    'n_test': widgets.IntText(value=default_config.data.n_test, description='n_test'),\n",
        "    'seed': widgets.IntText(value=default_config.data.seed, description='seed'),\n",
        "    'normalize_input': widgets.Checkbox(value=default_config.data.normalize_input, description='normalize_input'),\n",
        "}\n",
        "\n",
        "model_widgets = {\n",
        "    'hidden_sizes': widgets.Text(\n",
        "        value=','.join(map(str, default_config.model.hidden_sizes)),\n",
        "        description='hidden_sizes'\n",
        "    ),\n",
        "    'dropout_prob': widgets.FloatText(value=default_config.model.dropout_prob, description='dropout'),\n",
        "    'use_batch_norm': widgets.Checkbox(value=default_config.model.use_batch_norm, description='batch_norm'),\n",
        "}\n",
        "\n",
        "training_widgets = {\n",
        "    'batch_size': widgets.IntText(value=default_config.training.batch_size, description='batch_size'),\n",
        "    'learning_rate': widgets.FloatText(value=default_config.training.learning_rate, description='lr'),\n",
        "    'weight_decay': widgets.FloatText(value=default_config.training.weight_decay, description='weight_decay'),\n",
        "    'num_epochs': widgets.IntText(value=default_config.training.num_epochs, description='epochs'),\n",
        "    'early_stopping_patience': widgets.IntText(\n",
        "        value=default_config.training.early_stopping_patience,\n",
        "        description='early_stop'\n",
        "    ),\n",
        "    'eval_interval': widgets.IntText(value=default_config.training.eval_interval, description='eval_interval'),\n",
        "}\n",
        "\n",
        "eval_widgets = {\n",
        "    'top_m_values': widgets.Text(\n",
        "        value=','.join(map(str, default_config.eval.top_m_values)),\n",
        "        description='top_m_values'\n",
        "    )\n",
        "}\n",
        "\n",
        "comparison_widgets = {\n",
        "    'models_to_run': widgets.SelectMultiple(\n",
        "        options=sorted(MODEL_REGISTRY.keys()),\n",
        "        value=tuple(sorted(MODEL_REGISTRY.keys())),\n",
        "        description='models'\n",
        "    ),\n",
        "    'plots': widgets.SelectMultiple(\n",
        "        options=[\n",
        "            'training_history',\n",
        "            'eta_distribution',\n",
        "            'top_m_comparison',\n",
        "            'baseline_comparison',\n",
        "            'model_metric_bars',\n",
        "            'eta_boxplot',\n",
        "        ],\n",
        "        value=('model_metric_bars', 'eta_boxplot'),\n",
        "        description='plots'\n",
        "    ),\n",
        "    'compare_metrics': widgets.Text(\n",
        "        value='accuracy_top1,eta_top1,eta_top2',\n",
        "        description='metrics'\n",
        "    ),\n",
        "}\n",
        "\n",
        "tabs = widgets.Tab()\n",
        "tabs.children = [\n",
        "    widgets.VBox(list(system_widgets.values())),\n",
        "    widgets.VBox(list(data_widgets.values())),\n",
        "    widgets.VBox(list(model_widgets.values())),\n",
        "    widgets.VBox(list(training_widgets.values())),\n",
        "    widgets.VBox(list(eval_widgets.values())),\n",
        "    widgets.VBox(list(comparison_widgets.values())),\n",
        "]\n",
        "tabs.set_title(0, 'System')\n",
        "tabs.set_title(1, 'Data')\n",
        "tabs.set_title(2, 'Model')\n",
        "tabs.set_title(3, 'Training')\n",
        "tabs.set_title(4, 'Eval')\n",
        "tabs.set_title(5, 'Compare/Plots')\n",
        "\n",
        "run_button = widgets.Button(description='Run Experiments', button_style='success')\n",
        "output = widgets.Output()\n",
        "\n",
        "display(tabs, run_button, output)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- EXPERIMENT LOGIC ----\n",
        "def parse_int_list(text):\n",
        "    return [int(x.strip()) for x in text.split(',') if x.strip()]\n",
        "\n",
        "def parse_float_list(text):\n",
        "    return [float(x.strip()) for x in text.split(',') if x.strip()]\n",
        "\n",
        "def collect_config():\n",
        "    system = {k: w.value for k, w in system_widgets.items()}\n",
        "    data = {k: w.value for k, w in data_widgets.items()}\n",
        "    model = {k: w.value for k, w in model_widgets.items()}\n",
        "    training = {k: w.value for k, w in training_widgets.items()}\n",
        "    eval_cfg = {k: w.value for k, w in eval_widgets.items()}\n",
        "\n",
        "    model['hidden_sizes'] = parse_int_list(model['hidden_sizes'])\n",
        "    eval_cfg['top_m_values'] = parse_int_list(eval_cfg['top_m_values'])\n",
        "\n",
        "    comparison = {\n",
        "        'models_to_run': list(comparison_widgets['models_to_run'].value),\n",
        "        'plots': list(comparison_widgets['plots'].value),\n",
        "        'compare_metrics': [m.strip() for m in comparison_widgets['compare_metrics'].value.split(',') if m.strip()],\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        'system': system,\n",
        "        'data': data,\n",
        "        'model': model,\n",
        "        'training': training,\n",
        "        'eval': eval_cfg,\n",
        "        **comparison,\n",
        "    }\n",
        "\n",
        "def run_single_model(model_name, overrides):\n",
        "    config = get_config(\n",
        "        system=overrides['system'],\n",
        "        data=overrides['data'],\n",
        "        model=overrides['model'],\n",
        "        training=overrides['training'],\n",
        "        eval=overrides['eval'],\n",
        "    )\n",
        "\n",
        "    torch.manual_seed(config.data.seed)\n",
        "    np.random.seed(config.data.seed)\n",
        "\n",
        "    probe_bank = generate_probe_bank(\n",
        "        N=config.system.N,\n",
        "        K=config.system.K,\n",
        "        seed=config.data.seed,\n",
        "        phase_mode=config.system.phase_mode,\n",
        "        phase_bits=config.system.phase_bits,\n",
        "        probe_bank_method=config.system.probe_bank_method,\n",
        "    )\n",
        "\n",
        "    train_loader, val_loader, test_loader, metadata = create_dataloaders(config, probe_bank)\n",
        "    model = MODEL_REGISTRY[model_name](config)\n",
        "    model, history = train(model, train_loader, val_loader, config, metadata)\n",
        "\n",
        "    results = evaluate_model(\n",
        "        model,\n",
        "        test_loader,\n",
        "        config,\n",
        "        metadata['test_powers_full'],\n",
        "        metadata['test_labels'],\n",
        "        metadata['test_observed_indices'],\n",
        "        metadata['test_optimal_powers'],\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'name': model_name,\n",
        "        'model': model,\n",
        "        'history': history,\n",
        "        'results': results,\n",
        "        'config': config,\n",
        "    }\n",
        "\n",
        "def run_comparison(config_dict):\n",
        "    runs = []\n",
        "    for model_name in config_dict['models_to_run']:\n",
        "        runs.append(run_single_model(model_name, config_dict))\n",
        "    return runs\n",
        "\n",
        "def render_plots(runs, plots, compare_metrics):\n",
        "    for plot_name in plots:\n",
        "        if plot_name in {'model_metric_bars', 'eta_boxplot'}:\n",
        "            PLOT_REGISTRY[plot_name](runs, compare_metrics)\n",
        "        else:\n",
        "            for run in runs:\n",
        "                PLOT_REGISTRY[plot_name](run)\n",
        "\n",
        "def on_run_clicked(_):\n",
        "    output.clear_output()\n",
        "    with output:\n",
        "        config_dict = collect_config()\n",
        "        runs = run_comparison(config_dict)\n",
        "        render_plots(runs, config_dict['plots'], config_dict['compare_metrics'])\n",
        "\n",
        "run_button.on_click(on_run_clicked)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
