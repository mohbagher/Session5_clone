# ============================================================================
# COMPLETE FIXED VERSION: data/data_generation.py
# ============================================================================
"""
Data generation for RIS probe-based control with limited probing.

Key Features:
- HYBRID ENGINE: Automatically loads realistic channels from 'ris_channel_dataset.mat'
  if available (generated by MATLAB). Fallback to synthetic Rayleigh.
- ROBUST PROBE SELECTION: Supports 'random', 'oracle_topM', and 'ml_sequential'.
- FULL METADATA: Preserves all channel and oracle data for analysis.
- FIXED NORMALIZATION: Uses global max normalization to preserve relative magnitudes.
"""

import numpy as np
import os
import scipy.io as sio
from typing import Tuple, Dict, Optional, Callable, Union, List
import torch
from torch.utils.data import Dataset, DataLoader

from config.system_config import Config, SystemConfig
from data.probe_generators import ProbeBank


# --- HELPER: MATLAB LOADER ---
import h5py
import numpy as np

def load_matlab_channels(filepath: str, max_samples: int = None):
    """
    Load H_BR (h) and H_RU (g) from a MATLAB v7.3 .mat file.
    Enforces the shape (Samples, N).
    """
    print(f"  [Physics Engine] Loading 3GPP Standard channels from: {filepath}")
    try:
        with h5py.File(filepath, 'r') as f:
            # Reconstruct complex numbers
            h_data = f['H_BR_all']['real'][:] + 1j * f['H_BR_all']['imag'][:]
            g_data = f['H_RU_all']['real'][:] + 1j * f['H_RU_all']['imag'][:]

            # --- SHAPE CORRECTION LOGIC ---
            # MATLAB v7.3 stores (Samples, N) as (N, Samples) in Python.
            # We need (Samples, N) so that h[i] gives us a vector of length N.

            # If the current shape is (N, Samples), we MUST transpose it.
            # Example: If shape is (32, 10000), we flip it to (10000, 32)
            if h_data.shape[0] < h_data.shape[1]:
                h_data = h_data.T
                g_data = g_data.T

            # Now slice to the requested number of samples
            if max_samples:
                h_data = h_data[:max_samples]
                g_data = g_data[:max_samples]

        print(f"  [Physics Engine] Successfully loaded {h_data.shape[0]} samples with N={h_data.shape[1]}")
        return h_data, g_data

    except Exception as e:
        print(f"  [Physics Engine] Error loading MATLAB file: {e}")
        return None, None
# -----------------------------


def generate_channel_realization(N: int,
                                 sigma_h_sq: float = 1.0,
                                 sigma_g_sq: float = 1.0,
                                 rng: Optional[np.random.RandomState] = None) -> Tuple[np.ndarray, np.ndarray]:
    """Generate one channel realization (h, g) for Rayleigh fading (Fallback)."""
    if rng is None:
        rng = np.random.RandomState()
    h = np.sqrt(sigma_h_sq / 2) * (rng.randn(N) + 1j * rng.randn(N))
    g = np.sqrt(sigma_g_sq / 2) * (rng.randn(N) + 1j * rng.randn(N))
    return h, g


def compute_probe_powers(h: np.ndarray,
                         g: np.ndarray,
                         probe_bank: ProbeBank,
                         P_tx: float = 1.0) -> np.ndarray:
    """Compute received power for all probes given a channel realization."""
    c = h * g
    reflection_coeffs = probe_bank.get_reflection_coefficients()
    h_eff = np.dot(reflection_coeffs, c)
    powers = P_tx * np.abs(h_eff) ** 2
    return powers


def compute_optimal_power(h: np.ndarray, g: np.ndarray, P_tx: float = 1.0) -> float:
    """Compute theoretical optimal received power (Oracle)."""
    c = h * g
    h_eff_opt = np.sum(np.abs(c))
    P_opt = P_tx * h_eff_opt ** 2
    return P_opt


def select_probing_subset(K: int,
                          M: int,
                          rng: np.random.RandomState,
                          method: str = "random",
                          powers_full: Optional[np.ndarray] = None,
                          selector_fn: Optional[Callable] = None) -> np.ndarray:
    """
    Select M probe indices based on the specified method.

    Methods:
        - 'random': Random uniform selection without replacement.
        - 'oracle_topM': Selects the M probes with highest power (requires powers_full).
        - 'ml_sequential': Uses selector_fn to pick probes one by one.
    """
    if method == "random":
        indices = rng.choice(K, size=M, replace=False)
        return np.sort(indices)

    elif method == "oracle_topM":
        if powers_full is None:
            raise ValueError("powers_full required for oracle_topM")
        # Indices of M largest powers
        indices = np.argsort(powers_full)[-M:]
        return np.sort(indices)

    elif method == "ml_sequential":
        if selector_fn is None:
            raise ValueError("selector_fn required for ml_sequential")
        if powers_full is None:
            raise ValueError("powers_full required for sequential simulation")

        observed_indices = []

        # Sequential Loop
        for _ in range(M):
            # 1. Create masked input
            current_obs = np.array(observed_indices, dtype=np.int64)
            masked_p, mask = create_masked_input(powers_full, current_obs, K, normalize=True)

            # 2. Prepare Input for Selector (Handle Tensor conversion)
            # Input shape: [1, 2K] (batch size 1)
            model_input = np.concatenate([masked_p, mask])
            model_tensor = torch.FloatTensor(model_input).unsqueeze(0)

            # 3. Predict
            with torch.no_grad():
                # Selector can return:
                # A) Integer Index
                # B) Tensor/Array of Scores (Logits)
                prediction = selector_fn(model_tensor)

            if isinstance(prediction, torch.Tensor):
                prediction = prediction.squeeze().cpu().numpy()

            if np.ndim(prediction) == 0:
                # Case A: Scalar Index (Legacy/Simple)
                next_probe = int(prediction)
                # Fallback if duplicate
                if next_probe in observed_indices:
                     remaining = list(set(range(K)) - set(observed_indices))
                     if not remaining: break
                     next_probe = rng.choice(remaining)
            else:
                # Case B: Score Vector (Robust)
                scores = np.array(prediction)
                # Mask out already observed indices with -inf
                scores[observed_indices] = -float('inf')
                next_probe = int(np.argmax(scores))

            observed_indices.append(next_probe)

        return np.sort(np.array(observed_indices))

    else:
        raise ValueError(f"Unknown probe selection method: {method}")


def create_masked_input(
    powers_full: np.ndarray,
    observed_indices: np.ndarray,
    K: int,
    normalize: bool = True,
    normalization_method: str = 'max_global',
    global_max: Optional[float] = None
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Create the Neural Network input: [Masked_Powers, Binary_Mask]

    CRITICAL FIX: Normalization now preserves relative magnitude information.

    Args:
        powers_full: Full probe powers (K,)
        observed_indices: Observed probe indices (M,)
        K: Total number of probes
        normalize: Whether to normalize
        normalization_method: 'none', 'mean_sample' (BROKEN - backward compat only),
                             'max_sample', or 'max_global' (RECOMMENDED)
        global_max: Global max value (required for 'max_global')

    Returns:
        masked_powers: Masked power vector (K,) with zeros at unobserved positions
        mask: Binary mask (K,) with 1s at observed positions
    """
    masked_powers = np.zeros(K, dtype=np.float32)
    mask = np.zeros(K, dtype=np.float32)

    if len(observed_indices) == 0:
        return masked_powers, mask

    observed_powers = powers_full[observed_indices].copy()

    if normalize:
        if normalization_method == 'mean_sample':
            # OLD BROKEN METHOD - Only for backward compatibility
            # WARNING: This destroys relative magnitude information!
            mean_power = np.mean(observed_powers)
            if mean_power > 1e-10:
                observed_powers = observed_powers / mean_power

        elif normalization_method == 'max_sample':
            # Normalize by max observed power in this sample
            # Better than mean_sample, but still sample-dependent
            max_power = np.max(observed_powers)
            if max_power > 1e-10:
                observed_powers = observed_powers / max_power

        elif normalization_method == 'max_global':
            # RECOMMENDED: Normalize by global max across all samples
            # Preserves relative magnitudes between samples
            if global_max is None:
                raise ValueError("global_max required for max_global normalization")
            if global_max > 1e-10:
                observed_powers = observed_powers / global_max

        elif normalization_method == 'none':
            # No normalization - use raw power values
            pass

        else:
            raise ValueError(f"Unknown normalization method: {normalization_method}")

    masked_powers[observed_indices] = observed_powers
    mask[observed_indices] = 1.0

    return masked_powers, mask


def generate_limited_probing_dataset(
    probe_bank: ProbeBank,
    n_samples: int,
    M: int,
    system_config,
    normalize: bool = True,
    normalization_method: str = 'max_global',  # CHANGED: default to fixed method
    seed: Optional[int] = None,
    matlab_data: Optional[Tuple[np.ndarray, np.ndarray]] = None,
    selection_method: str = "random",
    selector_fn: Optional[Callable] = None
) -> Dict[str, np.ndarray]:
    """
    Generate dataset with FIXED normalization that preserves learning signal.

    CRITICAL FIX: Now uses two-pass approach:
    1. Generate all physics data first
    2. Compute global statistics
    3. Create masked inputs with proper normalization

    NEW PARAMETER:
        normalization_method: 'max_global' (recommended), 'max_sample',
                             'mean_sample' (old/broken), or 'none'
    """
    rng = np.random.RandomState(seed)
    K = probe_bank.K
    N = probe_bank.N

    # BACKWARD COMPATIBILITY: Handle dict or dataclass
    if isinstance(system_config, dict):
        cfg = system_config
    elif hasattr(system_config, '__dict__'):
        cfg = system_config.__dict__
    else:
        cfg = system_config

    # Extract parameters with defaults
    sigma_h_sq = cfg.get('sigma_h_sq', 1.0)
    sigma_g_sq = cfg.get('sigma_g_sq', 1.0)
    P_tx = cfg.get('P_tx', 1.0)

    # MATLAB Data Check
    h_matlab, g_matlab = None, None
    if matlab_data is not None:
        h_matlab, g_matlab = matlab_data
        if h_matlab.shape[0] < n_samples:
             pass # Will cycle

    # Allocate arrays
    masked_powers = np.zeros((n_samples, K), dtype=np.float32)
    masks = np.zeros((n_samples, K), dtype=np.float32)
    observed_indices = np.zeros((n_samples, M), dtype=np.int64)
    powers_full = np.zeros((n_samples, K), dtype=np.float32)
    labels = np.zeros(n_samples, dtype=np.int64)
    optimal_powers = np.zeros(n_samples, dtype=np.float32)

    # ========================================================================
    # CRITICAL FIX: Two-pass approach for global normalization
    # ========================================================================

    # PASS 1: Generate all physics data first
    for i in range(n_samples):
        # 1. Physics: Channel
        if h_matlab is not None:
             idx = i % h_matlab.shape[0]
             h = h_matlab[idx]
             g = g_matlab[idx]
        else:
             h, g = generate_channel_realization(
                N, sigma_h_sq, sigma_g_sq, rng
             )

        # 2. Physics: Full Powers (Oracle)
        p_full = compute_probe_powers(h, g, probe_bank, P_tx=P_tx)
        powers_full[i] = p_full

        # 3. Label & Optimal
        labels[i] = np.argmax(p_full)
        optimal_powers[i] = compute_optimal_power(h, g, P_tx=P_tx)

    # Compute global statistics for normalization
    global_max = np.max(powers_full) if normalize else None

    # PASS 2: Create masked inputs with proper normalization
    for i in range(n_samples):
        # 4. Probing Strategy
        obs_idx = select_probing_subset(
            K, M, rng,
            method=selection_method,
            powers_full=powers_full[i],
            selector_fn=selector_fn
        )
        observed_indices[i] = obs_idx

        # 5. NN Input with FIXED normalization
        mp, m = create_masked_input(
            powers_full[i],
            obs_idx,
            K,
            normalize=normalize,
            normalization_method=normalization_method,
            global_max=global_max
        )
        masked_powers[i] = mp
        masks[i] = m

    return {
        'masked_powers': masked_powers,
        'masks': masks,
        'observed_indices': observed_indices,
        'powers_full':  powers_full,
        'labels': labels,
        'optimal_powers': optimal_powers
    }


class LimitedProbingDataset(Dataset):
    """
    PyTorch Dataset that stores FULL metadata for analysis.
    """
    def __init__(self,
                 masked_powers: np.ndarray,
                 masks: np.ndarray,
                 labels: np.ndarray,
                 powers_full: Optional[np.ndarray] = None,
                 observed_indices: Optional[np.ndarray] = None,
                 optimal_powers: Optional[np.ndarray] = None):

        self.inputs = torch.FloatTensor(np.concatenate([masked_powers, masks], axis=1))
        self.labels = torch.LongTensor(labels)

        # Store metadata for detailed evaluation (Fix for Item 5)
        self.powers_full = powers_full
        self.observed_indices = observed_indices
        self.optimal_powers = optimal_powers
        self.masks = masks

    def __len__(self) -> int:
        return len(self.labels)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        return self.inputs[idx], self.labels[idx]


def create_dataloaders(config: Config,
                       probe_bank: ProbeBank,
                       selection_method: str = "random",
                       selector_fn: Optional[Callable] = None
                       ) -> Tuple[DataLoader, DataLoader, DataLoader, Dict]:
    """
    Create Datasets and Loaders.
    Passes full metadata and selection strategies down the chain.
    """
    data_config = config.data
    system_config = config.system
    training_config = config.training
    M = system_config.M

    print(f"\n[Data] Initializing Datasets (M={M}/{system_config.K})...")

    # MATLAB Check
    matlab_filename = "ris_channel_dataset.mat"
    matlab_path = None
    if os.path.exists(matlab_filename):
        matlab_path = matlab_filename
    elif os.path.exists(os.path.join("..", matlab_filename)):
        matlab_path = os.path.join("..", matlab_filename)

    matlab_data = None
    if matlab_path:
        print(f"[Data] Found verified MATLAB ground truth at: {matlab_path}")
        # We load the whole file. Slicing happens afterward.
        h_all, g_all = load_matlab_channels(matlab_path)
        if h_all is not None:
            matlab_data = (h_all, g_all)
    else:
        print("[Data] No MATLAB file found. Using internal synthetic generator (Rayleigh).")

    # --- SAFE-SLICING LOGIC ---
    train_mat, val_mat, test_mat = None, None, None
    if matlab_data:
        h_all, g_all = matlab_data
        n_available = h_all.shape[0]
        n_needed_total = data_config.n_train + data_config.n_val + data_config.n_test

        # If the MATLAB pool is large enough for unique splits, slice them.
        if n_available >= n_needed_total:
            n1 = data_config.n_train
            n2 = n1 + data_config.n_val
            train_mat = (h_all[:n1], g_all[:n1])
            val_mat = (h_all[n1:n2], g_all[n1:n2])
            test_mat = (h_all[n2:], g_all[n2:])
        else:
            # If the pool is too small, use the entire pool for every split.
            # The 'generate_limited_probing_dataset' function will use modulo
            # logic to cycle through the available samples without crashing.
            print(f"  [Data] Warning: MAT file size ({n_available}) < requested samples.")
            print("         Cycling the entire available pool for each split.")
            train_mat = (h_all, g_all)
            val_mat = (h_all, g_all)
            test_mat = (h_all, g_all)

    # Generate Datasets with FIXED normalization
    print("  Generating Training Set...")
    train_data = generate_limited_probing_dataset(
        probe_bank, data_config.n_train, M, system_config,
        normalize=data_config.normalize_input,
        normalization_method='max_global',  # ← FIXED
        seed=data_config.seed,
        matlab_data=train_mat,
        selection_method=selection_method,
        selector_fn=selector_fn
    )

    print("  Generating Validation Set...")
    val_data = generate_limited_probing_dataset(
        probe_bank, data_config.n_val, M, system_config,
        normalize=data_config.normalize_input,
        normalization_method='max_global',  # ← FIXED
        seed=data_config.seed + 1,
        matlab_data=val_mat,
        selection_method=selection_method,
        selector_fn=selector_fn
    )

    print("  Generating Test Set...")
    test_data = generate_limited_probing_dataset(
        probe_bank, data_config.n_test, M, system_config,
        normalize=data_config.normalize_input,
        normalization_method='max_global',  # ← FIXED
        seed=data_config.seed + 2,
        matlab_data=test_mat,
        selection_method=selection_method,
        selector_fn=selector_fn
    )

    # Create Dataset Objects
    train_dataset = LimitedProbingDataset(
        train_data['masked_powers'], train_data['masks'], train_data['labels'],
        powers_full=train_data['powers_full'],
        observed_indices=train_data['observed_indices'],
        optimal_powers=train_data['optimal_powers']
    )
    val_dataset = LimitedProbingDataset(
        val_data['masked_powers'], val_data['masks'], val_data['labels'],
        powers_full=val_data['powers_full'],
        observed_indices=val_data['observed_indices'],
        optimal_powers=val_data['optimal_powers']
    )
    test_dataset = LimitedProbingDataset(
        test_data['masked_powers'], test_data['masks'], test_data['labels'],
        powers_full=test_data['powers_full'],
        observed_indices=test_data['observed_indices'],
        optimal_powers=test_data['optimal_powers']
    )

    # Create Loaders
    train_loader = DataLoader(train_dataset, batch_size=training_config.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=training_config.batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=training_config.batch_size, shuffle=False)

    metadata = {
        'train_powers_full': train_data['powers_full'],
        'train_labels': train_data['labels'],
        'train_observed_indices': train_data['observed_indices'],
        'train_optimal_powers': train_data['optimal_powers'],

        'val_powers_full': val_data['powers_full'],
        'val_labels': val_data['labels'],
        'val_observed_indices': val_data['observed_indices'],
        'val_optimal_powers': val_data['optimal_powers'],

        'test_powers_full': test_data['powers_full'],
        'test_labels': test_data['labels'],
        'test_observed_indices': test_data['observed_indices'],
        'test_optimal_powers': test_data['optimal_powers'],
    }

    return train_loader, val_loader, test_loader, metadata